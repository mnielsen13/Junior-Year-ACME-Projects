{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514e4f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 1., 1., 0.]), 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "# Intialize P for test example\n",
    "#Left =0\n",
    "#Down = 1\n",
    "#Right = 2\n",
    "#Up= 3\n",
    "\n",
    "P = {s : {a: [] for a in range(4)} for s in range(4)}\n",
    "P[0][0] = [(0, 0, 0, False)]\n",
    "P[0][1] = [(1, 2, -1, False)]\n",
    "P[0][2] = [(1, 1, 0, False)]\n",
    "P[0][3] = [(0, 0, 0, False)]\n",
    "P[1][0] = [(1, 0, -1, False)]\n",
    "P[1][1] = [(1, 3, 1, True)]\n",
    "P[1][2] = [(0, 0, 0, False)]\n",
    "P[1][3] = [(0, 0, 0, False)]\n",
    "P[2][0] = [(0, 0, 0, False)]\n",
    "P[2][1] = [(0, 0, 0, False)]\n",
    "P[2][2] = [(1, 3, 1, True)]\n",
    "P[2][3] = [(1, 0, 0, False)]\n",
    "P[3][0] = [(0, 0, 0, True)]\n",
    "P[3][1] = [(0, 0, 0, True)]\n",
    "P[3][2] = [(0, 0, 0, True)]\n",
    "P[3][3] = [(0, 0, 0, True)]\n",
    "\n",
    "\n",
    "\n",
    "# Problem 1\n",
    "def value_iteration(P, nS ,nA, beta = 1, tol=1e-8, maxiter=3000):\n",
    "    \"\"\"Perform Value Iteration according to the Bellman optimality principle.\n",
    "\n",
    "    Parameters:\n",
    "        P (dict): The Markov relationship\n",
    "                (P[state][action] = [(prob, nextstate, reward, is_terminal)...]).\n",
    "        nS (int): The number of states.\n",
    "        nA (int): The number of actions.\n",
    "        beta (float): The discount rate (between 0 and 1).\n",
    "        tol (float): The stopping criteria for the value iteration.\n",
    "        maxiter (int): The maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "       v (ndarray): The discrete values for the true value function.\n",
    "       n (int): number of iterations\n",
    "    \"\"\"\n",
    "    V_old = np.zeros(nS)\n",
    "    for k in range(maxiter):              #only perform maxiter times\n",
    "        V_new = np.copy(V_old)\n",
    "        for s in range(nS):\n",
    "            sa_vector = np.zeros(nA)\n",
    "            for a in range(nA):\n",
    "                for tuple_info in P[s][a]:\n",
    "                    p, s_, u, urmom = tuple_info\n",
    "\n",
    "                    sa_vector[a] += (p * (u + beta * V_old[s_]))       #calculate the possible rewards of each action\n",
    "\n",
    "            V_new[s] = np.max(sa_vector)                #find the max reward\n",
    "        if np.linalg.norm((V_old - V_new)) < tol:       #check to see if the the V's are close enough to end\n",
    "            break\n",
    "        V_old = np.copy(V_new)                            #continue to the next step by letting Vk = Vk+1\n",
    "\n",
    "    return np.array(V_new), k+1\n",
    "\n",
    "def test1():\n",
    "    print(value_iteration(P, 4, 4))\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93d02c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Problem 2\n",
    "def extract_policy(P, nS, nA, v, beta = 1.0):\n",
    "    \"\"\"Returns the optimal policy vector for value function v\n",
    "\n",
    "    Parameters:\n",
    "        P (dict): The Markov relationship\n",
    "                (P[state][action] = [(prob, nextstate, reward, is_terminal)...]).\n",
    "        nS (int): The number of states.\n",
    "        nA (int): The number of actions.\n",
    "        v (ndarray): The value function values.\n",
    "        beta (float): The discount rate (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "        policy (ndarray): which direction to move in from each square.\n",
    "    \"\"\"\n",
    "    pi = np.zeros(nS)      #create the vector of policies\n",
    "    for s in range(nS):\n",
    "        action_r = np.zeros(nA)       #we will calculate the consequences of each possible action\n",
    "        for a  in range(nA):\n",
    "            for tuple_info in P[s][a]:\n",
    "                p, s_, u, urmom = tuple_info\n",
    "                action_r[a] += (p * (u + beta * v[s_]))           #calculate the results of a move from a given state\n",
    "\n",
    "        pi[s] = np.argmax(action_r)          #find the action that gives the greatest reward\n",
    "\n",
    "    return pi\n",
    "\n",
    "def test2():\n",
    "    v  = value_iteration(P, 4, 4)[0]\n",
    "    print(extract_policy(P, 4, 4, v, beta=1.0))\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e14404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Problem 3\n",
    "def compute_policy_v(P, nS, nA, policy, beta=1.0, tol=1e-8):\n",
    "    \"\"\"Computes the value function for a policy using policy evaluation.\n",
    "\n",
    "    Parameters:\n",
    "        P (dict): The Markov relationship\n",
    "                (P[state][action] = [(prob, nextstate, reward, is_terminal)...]).\n",
    "        nS (int): The number of states.\n",
    "        nA (int): The number of actions.\n",
    "        policy (ndarray): The policy to estimate the value function.\n",
    "        beta (float): The discount rate (between 0 and 1).\n",
    "        tol (float): The stopping criteria for the value iteration.\n",
    "\n",
    "    Returns:\n",
    "        v (ndarray): The discrete values for the true value function.\n",
    "    \"\"\"\n",
    "    go = True\n",
    "    V = np.zeros(nS)\n",
    "    while go is True:\n",
    "        V1 = np.zeros(nS)\n",
    "        for s in range(nS):          #iterate for each possible state\n",
    "            for tuple_info in P[s][policy[s]]:\n",
    "                p, s_, u, urmom = tuple_info\n",
    "                V1[s] += (p * (u + beta * V[s_]))        #use 11.7 to calculate the possible reward\n",
    "        if np.linalg.norm((V1 - V)) < tol:            #end if we are sufficiently close\n",
    "            break\n",
    "        V = np.copy(V1)\n",
    "\n",
    "    return V1\n",
    "\n",
    "def test3():\n",
    "    v  = value_iteration(P, 4, 4)[0]\n",
    "    policy = extract_policy(P, 4, 4, v, beta=1.0)\n",
    "    print(compute_policy_v(P, 4,4,policy))\n",
    "test3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75cdb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 1., 1., 0.]), array([2., 1., 2., 0.]), 2)\n"
     ]
    }
   ],
   "source": [
    "# Problem 4\n",
    "def policy_iteration(P, nS, nA, beta=1, tol=1e-8, maxiter=200):\n",
    "    \"\"\"Perform Policy Iteration according to the Bellman optimality principle.\n",
    "\n",
    "    Parameters:\n",
    "        P (dict): The Markov relationship\n",
    "                (P[state][action] = [(prob, nextstate, reward, is_terminal)...]).\n",
    "        nS (int): The number of states.\n",
    "        nA (int): The number of actions.\n",
    "        beta (float): The discount rate (between 0 and 1).\n",
    "        tol (float): The stopping criteria for the value iteration.\n",
    "        maxiter (int): The maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    \tv (ndarray): The discrete values for the true value function\n",
    "        policy (ndarray): which direction to move in each square.\n",
    "        n (int): number of iterations\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    pi0 = np.random.choice(nA, nS)      #initialize random policy vector\n",
    "    for k in range(maxiter):\n",
    "        V = compute_policy_v(P, nS, nA, pi0, beta, tol)       #use our previous functions in the given algorithm\n",
    "        pi1 = extract_policy(P, nS, nA, V, beta)\n",
    "\n",
    "        if np.linalg.norm((pi1 - pi0)) < tol:        #end if the two policy functions are sufficiently close\n",
    "            break\n",
    "\n",
    "        pi0 = pi1\n",
    "\n",
    "    return V, pi1, k\n",
    "\n",
    "def test4():\n",
    "    print(policy_iteration(P, 4, 4))\n",
    "test4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7982d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Problem 5 and 6\n",
    "def frozen_lake(basic_case=True, M=1000, render=False):\n",
    "    \"\"\" Finds the optimal policy to solve the FrozenLake problem\n",
    "\n",
    "    Parameters:\n",
    "    basic_case (boolean): True for 4x4 and False for 8x8 environemtns.\n",
    "    M (int): The number of times to run the simulation using problem 6.\n",
    "    render (boolean): Whether to draw the environment.\n",
    "\n",
    "    Returns:\n",
    "    vi_policy (ndarray): The optimal policy for value iteration.\n",
    "    vi_total_rewards (float): The mean expected value for following the value iteration optimal policy.\n",
    "    pi_value_func (ndarray): The maximum value function for the optimal policy from policy iteration.\n",
    "    pi_policy (ndarray): The optimal policy for policy iteration.\n",
    "    pi_total_rewards (float): The mean expected value for following the policy iteration optimal policy.\n",
    "    \"\"\"\n",
    "    if basic_case is True:\n",
    "        env_name = 'FrozenLake-v1'\n",
    "        env = gym.make(env_name).env\n",
    "        # Find number of states and actions\n",
    "        number_of_states = env.observation_space.n\n",
    "        number_of_actions = env.action_space.n\n",
    "        # Get the dictionary with all the states and actions\n",
    "        dictionary_P = env.P\n",
    "\n",
    "    else:\n",
    "        env_name = 'FrozenLake8x8-v1'\n",
    "        env = gym.make(env_name).env\n",
    "        # Find number of states and actions\n",
    "        number_of_states = env.observation_space.n\n",
    "        number_of_actions = env.action_space.n\n",
    "        # Get the dictionary with all the states and actions\n",
    "        dictionary_P = env.P\n",
    "\n",
    "    totrewvi = 0           #keep track of total rewards for mean reward\n",
    "    totrewpi = 0\n",
    "\n",
    "    vi_value_func, vi_iters = value_iteration(dictionary_P, number_of_states,number_of_actions, beta = 1.0, tol=1e-8, maxiter=3000) #use our function find the discrete values \n",
    "    vi_policy = extract_policy(dictionary_P, number_of_states, number_of_actions, vi_value_func, beta = 1.0)   #find the optimal policy by vi\n",
    "\n",
    "    for m in range(M):\n",
    "        rewardvi = run_simulation(env_name, vi_policy, render)     #run the simulation with vi policy and calculate the rewards\n",
    "        totrewvi += rewardvi\n",
    "\n",
    "    vi_total_rewards = totrewvi/M\n",
    "\n",
    "    pi_value_func, pi_policy, _ = policy_iteration(dictionary_P, number_of_states, number_of_actions, beta=1, tol=1e-8, maxiter=200)  #find optimal policy by pi\n",
    "    for m in range(M):\n",
    "        rewardpi = run_simulation(env_name, pi_policy, render)       #run the simulation with pi policy and calculate rewards\n",
    "        totrewpi += rewardpi\n",
    "\n",
    "    pi_total_rewards = totrewpi/M\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return vi_policy, vi_total_rewards, pi_value_func, pi_policy, pi_total_rewards\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Problem 6\n",
    "def run_simulation(env, policy, render=False, beta = 1.0):\n",
    "    \"\"\" Evaluates policy by using it to run a simulation and calculate the reward.\n",
    "\n",
    "    Parameters:\n",
    "    env (gym environment): The gym environment.\n",
    "    policy (ndarray): The policy used to simulate.\n",
    "    beta float: The discount factor.\n",
    "    render (boolean): Whether to draw the environment.\n",
    "\n",
    "    Returns:\n",
    "    total reward (float): Value of the total reward received under policy.\n",
    "    \"\"\"\n",
    "    env_name = env\n",
    "    done = False\n",
    "    env = gym.make(env_name).env\n",
    "    # Put environment in starting state\n",
    "    obs = env.reset()\n",
    "    totreward = 0\n",
    "    k = 0\n",
    "\n",
    "    while done is False:          #continue until simulation ends\n",
    "        obs, reward, done, _ = env.step(int(policy[obs]))      #take the step directed by our policy\n",
    "        if render is True: \n",
    "            env.render(mode = 'human')\n",
    "        totreward += (beta**k) * reward           #calculate the reward taking beta into account\n",
    "        k += 1\n",
    "\n",
    "    return totreward\n",
    "\n",
    "def test6():\n",
    "    policy = frozen_lake()[0]\n",
    "    print(run_simulation('FrozenLake-v1', policy, render=True))\n",
    "test6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569cb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
